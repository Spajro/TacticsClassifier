{
  "cells": [
    {
      "metadata": {
        "id": "9dfc98def8fb55f"
      },
      "cell_type": "markdown",
      "source": [
        "# LOADING DATASET"
      ],
      "id": "9dfc98def8fb55f"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install zstandard\n",
        "!pip install chess"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N75JnRl7ei5w",
        "outputId": "4650f21d-0d55-4f04-e7dd-2f85848ec934"
      },
      "id": "N75JnRl7ei5w",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: zstandard in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: chess in /usr/local/lib/python3.10/dist-packages (1.10.0)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:01.672675Z",
          "start_time": "2024-06-07T12:35:57.696212Z"
        },
        "id": "c4b51d4c10754771"
      },
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import urllib\n",
        "import zstandard\n",
        "import chess\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn"
      ],
      "id": "c4b51d4c10754771",
      "outputs": [],
      "execution_count": 12
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:01.677343Z",
          "start_time": "2024-06-07T12:36:01.673679Z"
        },
        "id": "c1af7d40bfdbe229"
      },
      "cell_type": "code",
      "source": [
        "def __download(url: str, name: str) -> str:\n",
        "    path, _ = urllib.request.urlretrieve(url, name)\n",
        "    return path\n",
        "\n",
        "\n",
        "def __unpack(path: str, name: str):\n",
        "    input_file = pathlib.Path(path)\n",
        "    with open(input_file, 'rb') as compressed:\n",
        "        decomp = zstandard.ZstdDecompressor()\n",
        "        output_path = name\n",
        "        with open(output_path, 'wb') as destination:\n",
        "            decomp.copy_stream(compressed, destination)\n",
        "            destination.close()\n",
        "        compressed.close()\n",
        "\n",
        "\n",
        "def __remove(path: str):\n",
        "    pathlib.Path.unlink(pathlib.Path(path))"
      ],
      "id": "c1af7d40bfdbe229",
      "outputs": [],
      "execution_count": 13
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:19.085888Z",
          "start_time": "2024-06-07T12:36:01.678592Z"
        },
        "id": "7b7f765e257cee1f"
      },
      "cell_type": "code",
      "source": [
        "path = __download(\"https://database.lichess.org/lichess_db_puzzle.csv.zst\", \"lichess_db_puzzle.csv.zst\")"
      ],
      "id": "7b7f765e257cee1f",
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.955800Z",
          "start_time": "2024-06-07T12:36:19.086891Z"
        },
        "id": "ca1de55ee484bb4e"
      },
      "cell_type": "code",
      "source": [
        "__unpack(path, \"lichess_db_puzzle.csv\")"
      ],
      "id": "ca1de55ee484bb4e",
      "outputs": [],
      "execution_count": 15
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.976709Z",
          "start_time": "2024-06-07T12:36:20.955800Z"
        },
        "id": "876c56ec9a7ac80d"
      },
      "cell_type": "code",
      "source": [
        "__remove(\"lichess_db_puzzle.csv.zst\")"
      ],
      "id": "876c56ec9a7ac80d",
      "outputs": [],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.980817Z",
          "start_time": "2024-06-07T12:36:20.976709Z"
        },
        "id": "initial_id"
      },
      "source": [
        "class Puzzle:\n",
        "    def __init__(self, row: str):\n",
        "        fields = row.split(',')\n",
        "        self.fen = fields[1]\n",
        "        self.moves = fields[2].split(\" \")\n",
        "        self.tags = fields[7].split(\" \")\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"{fen: \" + self.fen + \" ,tags: [\" + \", \".join(self.tags) + \"],moves: [\" + \",\".join(self.moves) + \"]}\""
      ],
      "outputs": [],
      "execution_count": 17
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.986533Z",
          "start_time": "2024-06-07T12:36:20.980817Z"
        },
        "id": "d76ee6cf90617af0"
      },
      "cell_type": "code",
      "source": [
        "def load(k: int) -> [Puzzle]:\n",
        "    f = open(\"lichess_db_puzzle.csv\")\n",
        "    f.readline()\n",
        "    result = []\n",
        "    for i in range(k):\n",
        "        result.append(Puzzle(f.readline()))\n",
        "    f.close()\n",
        "    return result"
      ],
      "id": "d76ee6cf90617af0",
      "outputs": [],
      "execution_count": 18
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.993161Z",
          "start_time": "2024-06-07T12:36:20.986533Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "57feb45f444a7d0a",
        "outputId": "04318c15-333b-48a1-c7e9-7250a00de6f5"
      },
      "cell_type": "code",
      "source": [
        "load(10)[0].__str__()"
      ],
      "id": "57feb45f444a7d0a",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'{fen: r6k/pp2r2p/4Rp1Q/3p4/8/1N1P2R1/PqP2bPP/7K b - - 0 24 ,tags: [crushing, hangingPiece, long, middlegame],moves: [f2g3,e6e7,b2b1,b3c1,b1c1,h6c1]}'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "execution_count": 19
    },
    {
      "metadata": {
        "id": "f362b99117932533"
      },
      "cell_type": "markdown",
      "source": [
        "# FILTER DATASET"
      ],
      "id": "f362b99117932533"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:20.997813Z",
          "start_time": "2024-06-07T12:36:20.994165Z"
        },
        "id": "54b9c024e9edbc45"
      },
      "cell_type": "code",
      "source": [
        "expected_tags = {\n",
        "    'attraction',\n",
        "    'discoveredAttack',\n",
        "    'doubleCheck',\n",
        "    'fork',\n",
        "    'pin',\n",
        "    'sacrifice',\n",
        "    'skewer',\n",
        "    'xRayAttack',\n",
        "    'zugzwang',\n",
        "    'deflection',\n",
        "    'clearance'\n",
        "}"
      ],
      "id": "54b9c024e9edbc45",
      "outputs": [],
      "execution_count": 20
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.003861Z",
          "start_time": "2024-06-07T12:36:20.999817Z"
        },
        "id": "b4c7b42dafb2af34"
      },
      "cell_type": "code",
      "source": [
        "expected_tags_list = list(expected_tags)"
      ],
      "id": "b4c7b42dafb2af34",
      "outputs": [],
      "execution_count": 21
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.009368Z",
          "start_time": "2024-06-07T12:36:21.003861Z"
        },
        "id": "68a75e47bbf20450"
      },
      "cell_type": "code",
      "source": [
        "def filter_data(data: [Puzzle]) -> [Puzzle]:\n",
        "    return list(filter(lambda p: len(set(p.tags) & expected_tags) == 1, data))"
      ],
      "id": "68a75e47bbf20450",
      "outputs": [],
      "execution_count": 22
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.017072Z",
          "start_time": "2024-06-07T12:36:21.010371Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed1341d0f35d0c64",
        "outputId": "156818ae-7e3c-4bec-9117-9c151b8f64e4"
      },
      "cell_type": "code",
      "source": [
        "len(filter_data(load(100)))"
      ],
      "id": "ed1341d0f35d0c64",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "execution_count": 23
    },
    {
      "metadata": {
        "id": "a5c92e35f0ad916d"
      },
      "cell_type": "markdown",
      "source": [
        "# CONVERSION TO TENSOR"
      ],
      "id": "a5c92e35f0ad916d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.021325Z",
          "start_time": "2024-06-07T12:36:21.018075Z"
        },
        "id": "10c35e85f53b2f3c"
      },
      "cell_type": "code",
      "source": [
        "def bitboard_to_tensor(bitboard: int) -> torch.Tensor:\n",
        "    li = [1 if digit == '1' else 0 for digit in bin(bitboard)[2:]]\n",
        "    li = [0 for _ in range(64 - len(li))] + li\n",
        "    return torch.tensor(li).reshape((8, 8))"
      ],
      "id": "10c35e85f53b2f3c",
      "outputs": [],
      "execution_count": 24
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.027740Z",
          "start_time": "2024-06-07T12:36:21.022328Z"
        },
        "id": "ac21f78c3f6922b4"
      },
      "cell_type": "code",
      "source": [
        "def fen_to_tensors_list(fen: str) -> [torch.Tensor]:\n",
        "    board = chess.Board(fen)\n",
        "    return [\n",
        "        bitboard_to_tensor(board.occupied_co[chess.WHITE]),\n",
        "        bitboard_to_tensor(board.occupied_co[chess.BLACK]),\n",
        "        bitboard_to_tensor(board.pawns),\n",
        "        bitboard_to_tensor(board.kings),\n",
        "        bitboard_to_tensor(board.queens),\n",
        "        bitboard_to_tensor(board.knights),\n",
        "        bitboard_to_tensor(board.bishops),\n",
        "        bitboard_to_tensor(board.rooks)\n",
        "    ]"
      ],
      "id": "ac21f78c3f6922b4",
      "outputs": [],
      "execution_count": 25
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.056343Z",
          "start_time": "2024-06-07T12:36:21.028744Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2af6bc1bbdd66c04",
        "outputId": "81a1b479-179c-431b-e806-9ce314ee73c0"
      },
      "cell_type": "code",
      "source": [
        "fen_to_tensors_list(load(1)[0].fen)"
      ],
      "id": "2af6bc1bbdd66c04",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 0, 0, 1, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 1, 0, 0, 1, 0, 1, 0],\n",
              "         [1, 1, 0, 0, 0, 1, 0, 1],\n",
              "         [1, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[1, 0, 0, 0, 0, 0, 0, 1],\n",
              "         [1, 0, 0, 1, 0, 0, 1, 1],\n",
              "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 1, 0, 0, 0, 1, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 0, 0, 0, 0, 0, 1, 1],\n",
              "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 1, 0, 0, 0],\n",
              "         [1, 1, 0, 0, 0, 1, 0, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[1, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 1, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 1, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 1],\n",
              "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 1, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 1, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0]])]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "execution_count": 26
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.060430Z",
          "start_time": "2024-06-07T12:36:21.057347Z"
        },
        "id": "17c3f21abe4260e5"
      },
      "cell_type": "code",
      "source": [
        "def move_to_tensor(move: str) -> torch.Tensor:\n",
        "    x1 = 7 - ord(move[0]) + ord('a')\n",
        "    y1 = 8 - int(move[1])\n",
        "    x2 = 7 - ord(move[2]) + ord('a')\n",
        "    y2 = 8 - int(move[3])\n",
        "    tensor = torch.zeros(8, 8)\n",
        "    tensor[y1][x1] = 1\n",
        "    tensor[y2][x2] = 1\n",
        "    return tensor"
      ],
      "id": "17c3f21abe4260e5",
      "outputs": [],
      "execution_count": 27
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.090045Z",
          "start_time": "2024-06-07T12:36:21.061433Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "104d99da6a271a6a",
        "outputId": "21506a1d-2a29-4cdd-e488-50cb80f4efe0"
      },
      "cell_type": "code",
      "source": [
        "print(move_to_tensor('e2e4'))"
      ],
      "id": "104d99da6a271a6a",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.094098Z",
          "start_time": "2024-06-07T12:36:21.091092Z"
        },
        "id": "7d58e2f64a04bec6"
      },
      "cell_type": "code",
      "source": [
        "def puzzle_to_tensor(puzzle: Puzzle) -> torch.Tensor:\n",
        "    fen_tensors = fen_to_tensors_list(puzzle.fen)\n",
        "    move_tensors = [move_to_tensor(puzzle.moves[0]), move_to_tensor(puzzle.moves[1])]  # FIRST TWO MOVES\n",
        "    return torch.stack(fen_tensors + move_tensors)"
      ],
      "id": "7d58e2f64a04bec6",
      "outputs": [],
      "execution_count": 29
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:36:21.106120Z",
          "start_time": "2024-06-07T12:36:21.095103Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f086ac45785396dd",
        "outputId": "3980bc24-bc8f-40a3-ed79-bb332171f27d"
      },
      "cell_type": "code",
      "source": [
        "puzzle_to_tensor(load(1)[0])"
      ],
      "id": "f086ac45785396dd",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 1., 0., 1., 0.],\n",
              "         [1., 1., 0., 0., 0., 1., 0., 1.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [1., 0., 0., 1., 0., 0., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 1., 1.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 1., 0., 0., 0.],\n",
              "         [1., 1., 0., 0., 0., 1., 0., 1.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 1., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0., 0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "execution_count": 30
    },
    {
      "metadata": {
        "id": "577ba77d6cc0369a"
      },
      "cell_type": "markdown",
      "source": [
        "# CONVERT AND BATCH DATASET"
      ],
      "id": "577ba77d6cc0369a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:47:13.034439Z",
          "start_time": "2024-06-07T12:47:13.031694Z"
        },
        "id": "f69689ed3c928f55"
      },
      "cell_type": "code",
      "source": [
        "def puzzle_to_truth(puzzle: Puzzle) -> torch.Tensor:\n",
        "    tensor = torch.zeros(len(expected_tags_list))\n",
        "    [tag] = set(puzzle.tags) & expected_tags\n",
        "    index = expected_tags_list.index(tag)\n",
        "    tensor[index] = 1\n",
        "    return torch.zeros(1) + index"
      ],
      "id": "f69689ed3c928f55",
      "outputs": [],
      "execution_count": 31
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:47:18.515569Z",
          "start_time": "2024-06-07T12:47:18.511299Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abd55187edfe5df",
        "outputId": "614276f1-a070-4ee3-c110-252922beb86f"
      },
      "cell_type": "code",
      "source": [
        "puzzle_to_truth(filter_data(load(100))[0])"
      ],
      "id": "abd55187edfe5df",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "execution_count": 32
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:47:18.791255Z",
          "start_time": "2024-06-07T12:47:18.788658Z"
        },
        "id": "bf6296834a2068eb"
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64"
      ],
      "id": "bf6296834a2068eb",
      "outputs": [],
      "execution_count": 33
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:47:19.077854Z",
          "start_time": "2024-06-07T12:47:19.075191Z"
        },
        "id": "2a98f0f8bbd52026"
      },
      "cell_type": "code",
      "source": [
        "def convert_dataset(puzzles: [Puzzle]) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
        "    return [(puzzle_to_tensor(puzzle), puzzle_to_truth(puzzle)) for puzzle in puzzles]"
      ],
      "id": "2a98f0f8bbd52026",
      "outputs": [],
      "execution_count": 34
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:49:49.313213Z",
          "start_time": "2024-06-07T12:49:49.310119Z"
        },
        "id": "37b1f86d84eb8476"
      },
      "cell_type": "code",
      "source": [
        "def dataset_to_batches(dataset: list[tuple[torch.Tensor, torch.Tensor]]) -> list[tuple[torch.Tensor, torch.Tensor]]:\n",
        "    batches = []\n",
        "    index = 0\n",
        "    while index + BATCH_SIZE <= len(dataset):\n",
        "        batch = []\n",
        "        truth = []\n",
        "        max_index = index + BATCH_SIZE\n",
        "        while index < max_index:\n",
        "            batch.append(dataset[index][0])\n",
        "            truth.append(dataset[index][1])\n",
        "            index += 1\n",
        "        batches.append((torch.stack(batch).cuda(), torch.tensor(truth).cuda().type(torch.long)))\n",
        "\n",
        "    return batches"
      ],
      "id": "37b1f86d84eb8476",
      "outputs": [],
      "execution_count": 35
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:49:50.731308Z",
          "start_time": "2024-06-07T12:49:50.109413Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c6ecc1932649601",
        "outputId": "18730c43-556d-410f-e342-621b9301d0d2"
      },
      "cell_type": "code",
      "source": [
        "batched_dataset=dataset_to_batches(convert_dataset(filter_data(load(10000))))\n",
        "print(len(batched_dataset))\n",
        "print(batched_dataset[0][0].shape,batched_dataset[0][1].shape)"
      ],
      "id": "2c6ecc1932649601",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n",
            "torch.Size([64, 10, 8, 8]) torch.Size([64])\n"
          ]
        }
      ],
      "execution_count": 36
    },
    {
      "metadata": {
        "id": "dd2e455b1a377348"
      },
      "cell_type": "markdown",
      "source": [
        "# TRAIN"
      ],
      "id": "dd2e455b1a377348"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:52:05.360344Z",
          "start_time": "2024-06-07T12:52:05.357737Z"
        },
        "id": "ab02ddd180fcdd69"
      },
      "cell_type": "code",
      "source": [
        "def accuracy(out,truth):\n",
        "    return torch.argmax(out,dim=1) == truth"
      ],
      "id": "ab02ddd180fcdd69",
      "outputs": [],
      "execution_count": 37
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:52:16.210248Z",
          "start_time": "2024-06-07T12:52:16.205903Z"
        },
        "id": "27c76f74edc787d2"
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(Model, self).__init__()\n",
        "        self.classifier = nn.Sequential(*args, **kwargs)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.classifier.forward(X)\n",
        "\n",
        "\n",
        "model = Model(nn.Conv2d(10, 8 * 8, kernel_size=4, padding=2),\n",
        "              nn.ReLU(),\n",
        "              nn.Conv2d(8 * 8, 4 * 4, kernel_size=2, padding=2),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool2d(kernel_size=4, stride=1),\n",
        "              nn.Conv2d(4*4, 8 * 8, kernel_size=2, padding=2),\n",
        "              nn.ReLU(),\n",
        "              nn.Conv2d(8 * 8, 1, kernel_size=4, padding=2),\n",
        "              nn.ReLU(),\n",
        "              nn.Flatten(),\n",
        "              nn.Linear(169, 256),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(256, 64),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(64, 11),\n",
        "              nn.LogSoftmax(),\n",
        "              )\n",
        "criterion = (\n",
        "    nn.NLLLoss()\n",
        ")\n"
      ],
      "id": "27c76f74edc787d2",
      "outputs": [],
      "execution_count": 38
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:49:55.105473Z",
          "start_time": "2024-06-07T12:49:55.103357Z"
        },
        "id": "230d17a6081a7ffc"
      },
      "cell_type": "code",
      "source": [
        "size_to_load=2000000\n",
        "test_batches_count=400"
      ],
      "id": "230d17a6081a7ffc",
      "outputs": [],
      "execution_count": 39
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:51:40.066389Z",
          "start_time": "2024-06-07T12:49:55.394152Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1a9a9709411156f",
        "outputId": "f7a517e3-9f44-4377-8805-f5084b25ffe9"
      },
      "cell_type": "code",
      "source": [
        "all_batches=dataset_to_batches(convert_dataset(filter_data(load(size_to_load))))\n",
        "train_batches=all_batches[test_batches_count:]\n",
        "test_batches=all_batches[:test_batches_count]\n",
        "print(len(all_batches),len(train_batches),len(test_batches))"
      ],
      "id": "d1a9a9709411156f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9421 9021 400\n"
          ]
        }
      ],
      "execution_count": 40
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:52:19.164920Z",
          "start_time": "2024-06-07T12:52:19.160747Z"
        },
        "id": "52588422f107c263"
      },
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, epoch):\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "    batches = train_batches\n",
        "    print(\"Dataset size:\", len(batches))\n",
        "    batch_index = 0\n",
        "    for i in range(epoch):\n",
        "\n",
        "        batch = batches[batch_index][0]\n",
        "        truth = batches[batch_index][1]\n",
        "\n",
        "        if batch_index == len(batches):\n",
        "            batch_index = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model.forward(batch)\n",
        "        loss = criterion(out, truth)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(loss.item(),(accuracy(out,truth).sum()/BATCH_SIZE).item())"
      ],
      "id": "52588422f107c263",
      "outputs": [],
      "execution_count": 41
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T12:57:37.570099Z",
          "start_time": "2024-06-07T12:52:19.424104Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b500d4f989b86b92",
        "outputId": "37b5159a-3cd7-4439-d896-511d69765228"
      },
      "cell_type": "code",
      "source": [
        "train(model,\n",
        "      criterion,\n",
        "      torch.optim.SGD(model.classifier.parameters(), lr=0.001),\n",
        "      200000)"
      ],
      "id": "b500d4f989b86b92",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 9021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.385636806488037 0.0625\n",
            "2.2460954189300537 0.28125\n",
            "2.1296308040618896 0.28125\n",
            "2.0187926292419434 0.28125\n",
            "1.801885962486267 0.28125\n",
            "1.767285704612732 0.28125\n",
            "1.7612438201904297 0.28125\n",
            "1.7575325965881348 0.28125\n",
            "1.7547497749328613 0.28125\n",
            "1.7524034976959229 0.28125\n",
            "1.7502433061599731 0.28125\n",
            "1.7480800151824951 0.28125\n",
            "1.7457209825515747 0.28125\n",
            "1.7429038286209106 0.28125\n",
            "1.739292860031128 0.28125\n",
            "1.7343196868896484 0.328125\n",
            "1.726558804512024 0.34375\n",
            "1.713478684425354 0.34375\n",
            "1.6857249736785889 0.359375\n",
            "1.6110093593597412 0.375\n",
            "1.3204820156097412 0.5\n",
            "0.7259989976882935 0.765625\n",
            "0.3731858730316162 0.890625\n",
            "0.1717609465122223 0.953125\n",
            "0.09680613875389099 0.96875\n",
            "0.058175452053546906 1.0\n",
            "0.03179274871945381 1.0\n",
            "0.016936713829636574 1.0\n",
            "0.00982736237347126 1.0\n",
            "0.006331420969218016 1.0\n",
            "0.004438151605427265 1.0\n",
            "0.003317667404189706 1.0\n",
            "0.0025987522676587105 1.0\n",
            "0.0021064120810478926 1.0\n",
            "0.0017533863428980112 1.0\n",
            "0.001491176662966609 1.0\n",
            "0.0012897215783596039 1.0\n",
            "0.0011315635638311505 1.0\n",
            "0.0010047969408333302 1.0\n",
            "0.0009011267102323472 1.0\n",
            "0.0008148571942001581 1.0\n",
            "0.0007422965718433261 1.0\n",
            "0.0006803671130910516 1.0\n",
            "0.0006270371377468109 1.0\n",
            "0.0005806948174722493 1.0\n",
            "0.0005401758244261146 1.0\n",
            "0.0005044334684498608 1.0\n",
            "0.00047271736548282206 1.0\n",
            "0.000444390723714605 1.0\n",
            "0.0004189379687886685 1.0\n",
            "0.0003959661698900163 1.0\n",
            "0.00037520923069678247 1.0\n",
            "0.00035628562909550965 1.0\n",
            "0.0003390217025298625 1.0\n",
            "0.00032318223384208977 1.0\n",
            "0.0003086266224272549 1.0\n",
            "0.00029522334807552397 1.0\n",
            "0.0002828390570357442 1.0\n",
            "0.00027136816061101854 1.0\n",
            "0.00026071781758219004 1.0\n",
            "0.0002507972239982337 1.0\n",
            "0.00024154322454705834 1.0\n",
            "0.00023289851378649473 1.0\n",
            "0.00022479254403151572 1.0\n",
            "0.00021719373762607574 1.0\n",
            "0.00021002217545174062 1.0\n",
            "0.00020329108519945294 1.0\n",
            "0.00019691108900588006 1.0\n",
            "0.00019090659043285996 1.0\n",
            "0.00018523108155932277 1.0\n",
            "0.00017986231250688434 1.0\n",
            "0.00017474441847298294 1.0\n",
            "0.00016990723088383675 1.0\n",
            "0.00016530434368178248 1.0\n",
            "0.00016090589633677155 1.0\n",
            "0.00015674361202400178 1.0\n",
            "0.00015276546764653176 1.0\n",
            "0.0001489546411903575 1.0\n",
            "0.0001453130244044587 1.0\n",
            "0.00014184435713104904 1.0\n",
            "0.0001384928764309734 1.0\n",
            "0.00013530880096368492 1.0\n",
            "0.00013225115253590047 1.0\n",
            "0.00012932562094647437 1.0\n",
            "0.00012651723227463663 1.0\n",
            "0.0001238074473803863 1.0\n",
            "0.00012122229964006692 1.0\n",
            "0.0001187189991469495 1.0\n",
            "0.00011631614324869588 1.0\n",
            "0.00011400446237530559 1.0\n",
            "0.00011176906991750002 1.0\n",
            "0.0001096155247068964 1.0\n",
            "0.00010753639071481302 1.0\n",
            "0.0001055261236615479 1.0\n",
            "0.00010358652798458934 1.0\n",
            "0.00010171022586291656 1.0\n",
            "9.990833495976403e-05 1.0\n",
            "9.814179793465883e-05 1.0\n",
            "9.644596138969064e-05 1.0\n",
            "9.480783774051815e-05 1.0\n",
            "9.321994002675638e-05 1.0\n",
            "9.166925156023353e-05 1.0\n",
            "9.016881813295186e-05 1.0\n",
            "8.872793841874227e-05 1.0\n",
            "8.731683919904754e-05 1.0\n",
            "8.592804806539789e-05 1.0\n",
            "8.459882519673556e-05 1.0\n",
            "8.329752745339647e-05 1.0\n",
            "8.204835467040539e-05 1.0\n",
            "8.081403211690485e-05 1.0\n",
            "7.96169406385161e-05 1.0\n",
            "7.844776700949296e-05 1.0\n",
            "7.731767982477322e-05 1.0\n",
            "7.621923577971756e-05 1.0\n",
            "7.513754826504737e-05 1.0\n",
            "7.408749661408365e-05 1.0\n",
            "7.308211934287101e-05 1.0\n",
            "7.209350587800145e-05 1.0\n",
            "7.112534512998536e-05 1.0\n",
            "7.018883479759097e-05 1.0\n",
            "6.925605703145266e-05 1.0\n",
            "6.835304520791396e-05 1.0\n",
            "6.749100430170074e-05 1.0\n",
            "6.663825479336083e-05 1.0\n",
            "6.580039917025715e-05 1.0\n",
            "6.49830253678374e-05 1.0\n",
            "6.418425618903711e-05 1.0\n",
            "6.341341941151768e-05 1.0\n",
            "6.264257535804063e-05 1.0\n",
            "6.191082502482459e-05 1.0\n",
            "6.117350130807608e-05 1.0\n",
            "6.0452905017882586e-05 1.0\n",
            "5.976397733320482e-05 1.0\n",
            "5.908063758397475e-05 1.0\n",
            "5.841589882038534e-05 1.0\n",
            "5.775861791335046e-05 1.0\n",
            "5.711808626074344e-05 1.0\n",
            "5.649430750054307e-05 1.0\n",
            "5.5870528740342706e-05 1.0\n",
            "5.529703412321396e-05 1.0\n",
            "5.470117321237922e-05 1.0\n",
            "5.412393875303678e-05 1.0\n",
            "5.355229222914204e-05 1.0\n",
            "5.300485645420849e-05 1.0\n",
            "5.2466715715127066e-05 1.0\n",
            "5.191925447434187e-05 1.0\n",
            "5.1397873903624713e-05 1.0\n",
            "5.090070408186875e-05 1.0\n",
            "5.039048846811056e-05 1.0\n",
            "4.990820525563322e-05 1.0\n",
            "4.940916187479161e-05 1.0\n",
            "4.8939913540380076e-05 1.0\n",
            "4.847252421313897e-05 1.0\n",
            "4.8031208280008286e-05 1.0\n",
            "4.756753332912922e-05 1.0\n",
            "4.7115026973187923e-05 1.0\n",
            "4.670349881052971e-05 1.0\n",
            "4.6265900891739875e-05 1.0\n",
            "4.584132693707943e-05 1.0\n",
            "4.541489397524856e-05 1.0\n",
            "4.501453440752812e-05 1.0\n",
            "4.461043863557279e-05 1.0\n",
            "4.4215663365321234e-05 1.0\n",
            "4.382647603051737e-05 1.0\n",
            "4.3444721086416394e-05 1.0\n",
            "4.30667023465503e-05 1.0\n",
            "4.269798591849394e-05 1.0\n",
            "4.233486106386408e-05 1.0\n",
            "4.196055306238122e-05 1.0\n",
            "4.1629085899330676e-05 1.0\n",
            "4.126968269702047e-05 1.0\n",
            "4.094007454114035e-05 1.0\n",
            "4.059369894093834e-05 1.0\n",
            "4.026781243737787e-05 1.0\n",
            "3.993820064351894e-05 1.0\n",
            "3.9617902075406164e-05 1.0\n",
            "3.928828300558962e-05 1.0\n",
            "3.899405783158727e-05 1.0\n",
            "3.868678322760388e-05 1.0\n",
            "3.838137854472734e-05 1.0\n",
            "3.808341716649011e-05 1.0\n",
            "3.779477265197784e-05 1.0\n",
            "3.749122333829291e-05 1.0\n",
            "3.721188477356918e-05 1.0\n",
            "3.693813778227195e-05 1.0\n",
            "3.6664387152995914e-05 1.0\n",
            "3.63869039574638e-05 1.0\n",
            "3.612805812736042e-05 1.0\n",
            "3.586361344787292e-05 1.0\n",
            "3.560475670383312e-05 1.0\n",
            "3.534403731464408e-05 1.0\n",
            "3.508704321575351e-05 1.0\n",
            "3.484867193037644e-05 1.0\n",
            "3.4589815186336637e-05 1.0\n",
            "3.4357035474386066e-05 1.0\n",
            "3.412424848647788e-05 1.0\n",
            "3.388028562767431e-05 1.0\n",
            "3.365309021319263e-05 1.0\n",
            "3.341844058013521e-05 1.0\n",
            "3.319682946312241e-05 1.0\n"
          ]
        }
      ],
      "execution_count": 42
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T13:03:32.223599Z",
          "start_time": "2024-06-07T13:03:32.218609Z"
        },
        "id": "1f69aaab85e1966a"
      },
      "cell_type": "code",
      "source": [
        "torch.save(model,'model.pt')"
      ],
      "id": "1f69aaab85e1966a",
      "outputs": [],
      "execution_count": 43
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T13:04:34.576061Z",
          "start_time": "2024-06-07T13:04:34.572931Z"
        },
        "id": "c8251ffcc9b16e94"
      },
      "cell_type": "code",
      "source": [
        "def test(model, criterion):\n",
        "    model.cuda()\n",
        "    criterion.cuda()\n",
        "    batches = test_batches\n",
        "    print(\"Dataset size:\", len(batches))\n",
        "    batch_index = 0\n",
        "\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    for i in range(len(batches)):\n",
        "\n",
        "        batch = batches[batch_index][0]\n",
        "        truth = batches[batch_index][1]\n",
        "\n",
        "        if batch_index == len(batches):\n",
        "            batch_index = 0\n",
        "\n",
        "        out = model.forward(batch)\n",
        "        loss = criterion(out, truth)\n",
        "        print(loss.item(),(accuracy(out,truth).sum()/BATCH_SIZE).item())\n",
        "        total_loss += loss.item()\n",
        "        total_accuracy+=(accuracy(out,truth).sum()/BATCH_SIZE).item()\n",
        "\n",
        "    return (total_loss / len(batches)),total_accuracy / len(batches)"
      ],
      "id": "c8251ffcc9b16e94",
      "outputs": [],
      "execution_count": 44
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T13:04:35.187407Z",
          "start_time": "2024-06-07T13:04:34.785399Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9a5368131bcb1d",
        "outputId": "643715a9-4896-437d-ff0a-ffb61c93c7d3"
      },
      "cell_type": "code",
      "source": [
        "test(model, criterion)"
      ],
      "id": "ac9a5368131bcb1d",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size: 400\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n",
            "18.07448387145996 0.125\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(18.07448387145996, 0.125)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "execution_count": 45
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-06-07T13:03:41.259991Z",
          "start_time": "2024-06-07T13:03:41.253014Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3810482fe9e960bf",
        "outputId": "fa6735d4-342f-4aec-925b-6d0435c4f0f3"
      },
      "cell_type": "code",
      "source": [
        "def eye_test(model,puzzle):\n",
        "    tensor=puzzle_to_tensor(puzzle).cuda()\n",
        "    out=model.forward(tensor)\n",
        "    return sorted(zip(expected_tags_list,out.squeeze().tolist()),key=lambda x:-x[1]),puzzle.tags\n",
        "\n",
        "eye_test(model,filter_data(load(100))[0])"
      ],
      "id": "3810482fe9e960bf",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([('discoveredAttack', -0.2919864356517792),\n",
              "  ('fork', -1.581228256225586),\n",
              "  ('deflection', -3.0470447540283203),\n",
              "  ('skewer', -18.6107234954834),\n",
              "  ('pin', -27.099401473999023),\n",
              "  ('attraction', -36.285396575927734),\n",
              "  ('zugzwang', -37.41444778442383),\n",
              "  ('sacrifice', -39.04969787597656),\n",
              "  ('clearance', -42.260833740234375),\n",
              "  ('doubleCheck', -74.36854553222656),\n",
              "  ('xRayAttack', -80.13048553466797)],\n",
              " ['crushing', 'endgame', 'exposedKing', 'long', 'skewer'])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "execution_count": 46
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}